#!/usr/bin/env ruby

require 'scout'
require 'scout-ai'

$0 = "scout #{$previous_commands.any? ? $previous_commands*" " + " " : "" }#{ File.basename(__FILE__) }" if $previous_commands

options = SOPT.setup <<EOF

Ask GPT

$ #{$0} [<options>] [question]

Use STDIN to add context to the question

-h--help Print this help
-l--log* Log level
-t--template* Use a template
-c--chat* Follow a conversation
-m--model* Model to use
-e--endpoint* Endpoint to use
-f--file* Incorporate file at the start
EOF
if options[:help]
  if defined? scout_usage
    scout_usage 
  else
    puts SOPT.doc
  end
  exit 0
end

Log.severity = options.delete(:log).to_i if options.include? :log

file, chat, template = IndiferentHash.process_options options, :file, :chat, :template

question = ARGV * " "

if template
  if Open.exists?(template)
    template_question = Open.read(template)
  else
    template_question = Scout.questions[template].read
  end
  if template_question.include?('???')
    question = template_question.sub('???', question)
  else
    question = template_question
  end
end

if question.include?('...')
  context = file ? Open.read(file) : STDIN.read
  question = question.sub('...', context)
elsif file
  question = "<file basename=#{File.basename file}>[[[\n" + Open.read(file) + "\n]]]</file>"
end

if chat
  conversation = Open.exist?(chat)? Open.read(chat) : "" 
  question = question.empty? ? conversation : conversation + "\nuser:\n" +  question
  new = LLM.ask(question, options)
  conversation = question + "\nassistant:\n" + new
  Open.write(chat, conversation)
else
  puts LLM.ask(question, options)
end
