#!/usr/bin/env ruby

require 'scout'
require 'scout-ai'

$0 = "scout #{$previous_commands.any? ? $previous_commands*" " + " " : "" }#{ File.basename(__FILE__) }" if $previous_commands

options = SOPT.setup <<EOF

Ask GPT

$ #{$0} [<options>] [question]

Use STDIN to add context to the question

-h--help Print this help
-l--log* Log level
-t--template* Use a template
-c--chat* Follow a conversation
-m--model* Model to use
-e--endpoint* Endpoint to use
-b--backend* Backend to use
-f--file* Incorporate file at the start
-d--dry_run Dry run, don't ask
EOF
if options[:help]
  if defined? scout_usage
    scout_usage 
  else
    puts SOPT.doc
  end
  exit 0
end

Log.severity = options.delete(:log).to_i if options.include? :log

file, chat, template, dry_run = IndiferentHash.process_options options, :file, :chat, :template, :dry_run

question = ARGV * " "

if template
  if Open.exists?(template)
    template_question = Open.read(template)
  else
    template_question = Scout.questions[template].read
  end
  if template_question.include?('???')
    question = template_question.sub('???', question)
  else
    question = template_question
  end
end

if question.include?('...')
  context = file ? Open.read(file) : STDIN.read
  question = question.sub('...', context)
elsif file
  question = "<file basename=#{File.basename file}>[[[\n" + Open.read(file) + "\n]]]</file>"
end

if chat
  conversation = Open.exist?(chat)? LLM.chat(chat) : [] 
  convo_options = LLM.options conversation
  conversation = question.empty? ? conversation : conversation +  LLM.chat(question)
  if dry_run
    ppp LLM.print conversation
    exit 0
  end
  new = LLM.ask(conversation, convo_options.merge(options.merge(return_messages: true)))
  conversation = Open.read(chat) + LLM.print(new)
  Open.write(chat, conversation)
else
  puts LLM.ask(question, options)
end
